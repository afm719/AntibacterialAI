{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ccf547",
   "metadata": {},
   "source": [
    "# Documentation: Generative Variational Autoencoder (VAE) for Molecular SMILES Generation\n",
    "\n",
    "The notebook _generativeVAEV1_ implements a Variational Autoencoder (VAE) designed to learn a compressed representation of molecular structures (in SMILES format) and then use this learned representation to generate novel, chemically valid SMILES strings. \n",
    "\n",
    "## 1. Installing Dependencies why we are using RDKit\n",
    "\n",
    "-   **RDKit**: A fundamental open-source cheminformatics toolkit. It's imp for this project as it allows us to:\n",
    "    -   Parse and interpret SMILES strings (Simplified Molecular Input Line Entry System), which are linear notations for molecular structures.\n",
    "    -   Validate whether a generated SMILES string corresponds to a chemically plausible molecule.\n",
    "    -   Perform various other molecular computations if needed (though primarily used for validation here).\n",
    "\n",
    "If RDKit is already installed in your environment, this command will simply confirm its presence.\n",
    "\n",
    "## 2. The activity of each library\n",
    "\n",
    "*(Corresponds to the cell with `import numpy as np`, etc.)*\n",
    "\n",
    "This cell imports all the Python libraries and modules required for the notebook's functionality:\n",
    "\n",
    "-   **`numpy` (as `np`)**: A cornerstone library for numerical computing in Python. It's used for efficient array operations, especially for handling the numerical data fed into and produced by the neural network.\n",
    "-   **`pandas` (as `pd`)**: A powerful library for data manipulation and analysis. Here, it's primarily used to read the input dataset from a CSV file into a DataFrame.\n",
    "-   **`tensorflow` (as `tf`)**: The core deep learning framework.\n",
    "    -   **`keras` from `tensorflow`**: TensorFlow's high-level API for building and training neural networks.\n",
    "    -   **`layers` from `tensorflow.keras`**: Contains the building blocks for neural networks.\n",
    "    -   **`Model` from `tensorflow.keras`**: The class used to define a Keras model.\n",
    "    -   **`Adam` from `tensorflow.keras.optimizers`**: An efficient gradient-based optimization algorithm.\n",
    "    -   **`SparseCategoricalCrossentropy` from `tensorflow.keras.losses`**: A loss function suitable for multi-class classification problems where labels are integers.\n",
    "    -   **`plot_model` from `tensorflow.keras.utils`**: A utility to create a visual plot of the Keras model architecture.\n",
    "    -   **`ModelCheckpoint`, `TensorBoard` from `tensorflow.keras.callbacks`**: Utilities that can be applied at different stages of model training.\n",
    "-   **`Chem` from `rdkit`**: The core RDKit module for working with chemical structures.\n",
    "-   **`os`**: For interacting with the operating system (e.g., creating directories).\n",
    "-   **`pickle`**: For serializing and de-serializing Python objects (saving/loading the tokenizer).\n",
    "-   **`matplotlib.pyplot` (as `plt`)**: A widely used plotting library.\n",
    "\n",
    "## 3. Defining Constants and Hyperparameters\n",
    "\n",
    "*(Corresponds to the cell defining `DATA_PATH`, `LATENT_DIM`, etc.)*\n",
    "\n",
    "This section defines global constants for file paths and hyperparameters:\n",
    "\n",
    "### File Paths:\n",
    "-   **`DATA_PATH`**: Location of the input CSV file with SMILES strings.\n",
    "-   **`TOKENIZER_PATH`**: Path to save/load the SMILES tokenizer.\n",
    "-   **`MODEL_PATH`**: Base directory to save trained models.\n",
    "-   **`LOGS_PATH`**: Directory for TensorBoard logs.\n",
    "\n",
    "### Hyperparameters:\n",
    "-   **`LATENT_DIM`**: Dimensionality of the VAE's latent space (e.g., 128). This determines the \"bottleneck\" size.\n",
    "-   **`BATCH_SIZE`**: Number of samples processed before model weights are updated (e.g., 256).\n",
    "-   **`EPOCHS`**: Number of full passes through the training dataset (e.g., 100).\n",
    "-   **`MAX_LENGTH`**: Maximum length for SMILES strings after tokenization and padding (e.g., 120).\n",
    "\n",
    "## 4. Creating Necessary Directories\n",
    "\n",
    "*(Corresponds to the cell with `os.makedirs(...)`)*\n",
    "\n",
    "This cell creates the directories for storing the tokenizer, models, logs, and generated molecules if they don't already exist. This prevents errors during file saving operations.\n",
    "\n",
    "## 5. Loading the Dataset\n",
    "\n",
    "*(Corresponds to the cell with `pd.read_csv(DATA_PATH)`)*\n",
    "\n",
    "The molecular data (SMILES strings) is loaded from a CSV file using pandas. It's assumed the CSV has a column named 'smiles'.\n",
    "\n",
    "## 6. SMILES Tokenizer Class Definition\n",
    "\n",
    "*(Corresponds to the `SMILESTokenizer` class definition)*\n",
    "\n",
    "Neural networks require numerical input. This `SMILESTokenizer` class handles the conversion of textual SMILES strings into numerical sequences (tokenization) and back (detokenization).\n",
    "\n",
    "### `__init__(self, max_length)`\n",
    "-   Initializes dictionaries for character-to-integer (`char_to_int`) and integer-to-character (`int_to_char`) mappings.\n",
    "-   Sets `max_length`.\n",
    "-   Defines special tokens:\n",
    "    -   `<pad>`: For padding shorter sequences.\n",
    "    -   `<start>`: To mark the beginning of a sequence.\n",
    "    -   `<end>`: To mark the end of a sequence.\n",
    "    -   `<unk>`: For unknown characters not in the vocabulary.\n",
    "\n",
    "### `fit(self, smiles_list)`\n",
    "-   Builds the vocabulary from a list of SMILES strings.\n",
    "-   Collects all unique characters and adds special tokens.\n",
    "-   Creates `char_to_int` and `int_to_char` mappings.\n",
    "-   Sets `vocab_size` (total number of unique tokens).\n",
    "\n",
    "### `transform(self, smiles_list)`\n",
    "-   Converts a list of SMILES strings into a NumPy array of padded integer sequences.\n",
    "-   Each SMILES string is:\n",
    "    1.  Prepended with `<start>` token.\n",
    "    2.  Characters are mapped to integers (or `<unk>` if not in vocab).\n",
    "    3.  Appended with `<end>` token.\n",
    "    4.  Padded with `<pad>` token or truncated to `max_length`.\n",
    "\n",
    "### `reverse_transform(self, sequences)`\n",
    "-   Converts integer sequences back to SMILES strings.\n",
    "-   For each sequence, integers are mapped back to characters, stopping at `<end>` or `<pad>`. `<start>` and `<unk>` tokens are not included in the final string.\n",
    "\n",
    "### `save(self, filepath)` & `load(filepath)`\n",
    "-   Methods to save and load the tokenizer object using `pickle`, allowing reuse without refitting.\n",
    "\n",
    "## 7. Initializing, Fitting, and Saving the Tokenizer\n",
    "\n",
    "*(Corresponds to `tokenizer = SMILESTokenizer(...)`, `tokenizer.fit(...)`, `tokenizer.save(...)`)*\n",
    "\n",
    "1.  An instance of `SMILESTokenizer` is created.\n",
    "2.  The tokenizer is `fit` to the loaded `smiles_data` to build its vocabulary.\n",
    "3.  The fitted tokenizer is saved to disk for later use.\n",
    "4.  The vocabulary size is printed.\n",
    "\n",
    "## 8. Transforming SMILES Data into Tokenized Sequences\n",
    "\n",
    "*(Corresponds to `data_tokenized = tokenizer.transform(smiles_data)`)*\n",
    "\n",
    "The `smiles_data` (list of SMILES strings) is converted into `data_tokenized` (a NumPy array of integer sequences) using the fitted tokenizer. This numerical data will be the input to the VAE.\n",
    "\n",
    "## 9. Defining the Variational Autoencoder (VAE) Model\n",
    "\n",
    "*(Corresponds to the VAE model definition, including Encoder, Sampling, and Decoder)*\n",
    "\n",
    "A VAE learns a probabilistic mapping from input data to a lower-dimensional continuous latent space, and then back to the input space.\n",
    "\n",
    "### 9.1. Sampling Layer (`Sampling` class)\n",
    "The VAE encoder outputs parameters of a distribution (mean $z_{\\mu}$ and log-variance $z_{\\log \\sigma^2}$) for each input. The `Sampling` layer draws a sample $z$ from this distribution using the **reparameterization trick**:\n",
    "$$ z = z_{\\mu} + \\exp(0.5 \\cdot z_{\\log \\sigma^2}) \\cdot \\epsilon $$\n",
    "where $\\epsilon$ is a random sample from a standard normal distribution $N(0, I)$. This allows gradients to flow through the stochastic sampling process.\n",
    "\n",
    "### 9.2. Encoder Network\n",
    "Maps an input tokenized SMILES sequence to the latent distribution parameters $z_{\\mu}$ and $z_{\\log \\sigma^2}$.\n",
    "-   **`Input`**: Defines input shape (`MAX_LENGTH`).\n",
    "-   **`Embedding`**: Converts integer tokens into dense vectors of fixed size (e.g., 128 dimensions). `mask_zero=True` handles padding by ignoring zero-indexed pad tokens in subsequent layers like LSTMs.\n",
    "-   **`LSTM`**: Long Short-Term Memory layers process sequential data. Two LSTM layers are used here to capture features from the embedded sequences. The first LSTM has `return_sequences=True` to pass its full output sequence to the next LSTM. The second LSTM outputs only the final hidden state.\n",
    "-   **`Dense` layers for $z_{\\mu}$ and $z_{\\log \\sigma^2}$**: Fully connected layers that map the LSTM output to the `LATENT_DIM`-dimensional mean and log-variance vectors.\n",
    "\n",
    "### 9.3. Decoder Network\n",
    "Maps a sampled latent vector $z$ back to a sequence of tokens, aiming to reconstruct the input SMILES.\n",
    "-   **`Input`**: Defines input shape (`LATENT_DIM` for the latent vector $z$).\n",
    "-   **`RepeatVector(MAX_LENGTH)`**: Repeats the latent vector $z$ `MAX_LENGTH` times to provide an initial sequence input for the decoder's LSTMs.\n",
    "-   **`LSTM`**: Two LSTM layers (with `return_sequences=True` for both) generate an output sequence from the repeated latent vector.\n",
    "-   **`TimeDistributed(Dense)`**: Applies a `Dense` layer to every time step of the LSTM output. This layer has `tokenizer.vocab_size` units and `softmax` activation, producing a probability distribution over the vocabulary for each token in the output sequence.\n",
    "\n",
    "### 9.4. VAE Model (Combined)\n",
    "Connects the encoder, sampling layer, and decoder.\n",
    "-   Input SMILES $\\rightarrow$ Encoder $\\rightarrow (z_{\\mu}, z_{\\log \\sigma^2})$\n",
    "-   $(z_{\\mu}, z_{\\log \\sigma^2}) \\rightarrow$ Sampling layer $\\rightarrow z$\n",
    "-   $z \\rightarrow$ Decoder $\\rightarrow$ Reconstructed SMILES (as probability distributions over tokens)\n",
    "\n",
    "### VAE Loss Function\n",
    "The VAE is trained by minimizing a loss function comprising two terms:\n",
    "1.  **Reconstruction Loss**: Measures how well the VAE reconstructs the input.\n",
    "    -   Uses `SparseCategoricalCrossentropy` because the decoder outputs probability distributions for token classes, and the target is integer token indices.\n",
    "    $$ L_{\\text{reconstruction}} = -\\sum_{t=1}^{T} \\log p(x_t | z) $$\n",
    "    (Typically implemented as cross-entropy over the sequence.)\n",
    "\n",
    "2.  **KL Divergence (KLD) Loss**: A regularization term that encourages the learned latent distribution $q(z|x)$ (from the encoder) to be close to a prior distribution $p(z)$ (typically a standard normal $N(0, I)$). This helps create a smooth and continuous latent space useful for generation.\n",
    "    The KL divergence between the learned distribution $q(z|x) = N(z | z_{\\mu}(x), \\text{diag}(\\exp(z_{\\log \\sigma^2}(x))))$ and the prior $p(z) = N(z | 0, I)$ is given by:\n",
    "    $$ D_{KL}(q(z|x) || p(z)) = -0.5 \\sum_{j=1}^{\\text{LATENT\\_DIM}} (1 + z_{\\log \\sigma^2_j} - (z_{\\mu_j})^2 - \\exp(z_{\\log \\sigma^2_j})) $$\n",
    "    This term is added to the total loss, often weighted by a factor $\\beta$. The `vae.add_loss(kl_loss * 0.1)` line in the code implements this, where `kl_loss` is the negative of the sum above, averaged over the batch.\n",
    "\n",
    "The total loss function (actually the negative of the Evidence Lower Bound, ELBO) is:\n",
    "$$ L_{\\text{VAE}} = L_{\\text{reconstruction}} + \\beta \\cdot D_{KL}(q(z|x) || p(z)) $$\n",
    "\n",
    "## 10. Compiling the VAE Model\n",
    "\n",
    "*(Corresponds to `vae.compile(...)`)*\n",
    "\n",
    "The VAE model is compiled, specifying:\n",
    "-   **`optimizer=Adam(learning_rate=0.001)`**: The Adam optimization algorithm.\n",
    "-   **`loss=reconstruction_loss_fn`**: The primary loss function (SparseCategoricalCrossentropy). The KL divergence was added via `vae.add_loss()` and is automatically included by Keras.\n",
    "\n",
    "## 11. Defining Callbacks for Training\n",
    "\n",
    "*(Corresponds to the `callbacks = [...]` list)*\n",
    "\n",
    "Callbacks monitor and influence training:\n",
    "-   **`ModelCheckpoint`**: Saves the model (or best version based on a monitored metric like `loss` or `val_loss`) during training.\n",
    "-   **`TensorBoard`**: Logs training metrics (loss, etc.) for visualization with TensorBoard.\n",
    "\n",
    "## 12. Training the VAE Model\n",
    "\n",
    "*(Corresponds to `history = vae.fit(...)`)*\n",
    "\n",
    "The VAE is trained using the `fit` method:\n",
    "-   Input data: `data_tokenized`.\n",
    "-   Target data: `data_tokenized` (since it's an autoencoder structure trying to reconstruct its input).\n",
    "-   `epochs`, `batch_size`, and `callbacks` are passed as arguments.\n",
    "-   An optional `validation_split` could be used to monitor performance on a held-out validation set.\n",
    "\n",
    "## 13. Saving the Trained Model Components\n",
    "\n",
    "*(Corresponds to `encoder.save(...)`, `decoder.save(...)`, `vae.save(...)`)*\n",
    "\n",
    "After training, the encoder, decoder, and the full VAE model are saved to disk. This allows them to be loaded later for inference or further training.\n",
    "\n",
    "## 14. Utility Functions for SMILES Generation and Validation\n",
    "\n",
    "*(Corresponds to `generate_smiles_from_latent_space` and `is_valid_smiles` function definitions)*\n",
    "\n",
    "### `generate_smiles_from_latent_space(...)`\n",
    "-   Generates new SMILES strings:\n",
    "    1.  Samples random vectors from the latent space (typically $N(0, I)$).\n",
    "    2.  Feeds these vectors to the trained `decoder`.\n",
    "    3.  The decoder outputs token probability distributions for each position in the sequence.\n",
    "    4.  `np.argmax()` selects the most probable token at each position (greedy decoding).\n",
    "    5.  The resulting token index sequences are converted back to SMILES strings using `tokenizer.reverse_transform()`.\n",
    "\n",
    "### `is_valid_smiles(smiles)`\n",
    "-   Checks chemical validity of a SMILES string using RDKit's `Chem.MolFromSmiles(smiles)`.\n",
    "-   Returns `True` if the SMILES string can be parsed into a valid molecule object, `False` otherwise.\n",
    "\n",
    "## 15. Generating and Filtering New SMILES Strings\n",
    "\n",
    "*(Corresponds to the cell generating and filtering SMILES with `N_SAMPLES_TO_GENERATE`)*\n",
    "\n",
    "-   A specified number of raw SMILES strings are generated using `generate_smiles_from_latent_space`.\n",
    "-   Each generated SMILES string is then validated using `is_valid_smiles`.\n",
    "-   Only valid SMILES strings are kept.\n",
    "-   The code also includes a step to find unique valid SMILES strings.\n",
    "\n",
    "## 16. Displaying a Sample of Generated SMILES\n",
    "\n",
    "*(Corresponds to the cell printing the first 100 generated SMILES)*\n",
    "\n",
    "A sample of the generated unique and valid SMILES strings is printed to visually inspect the quality of the generated molecules.\n",
    "\n",
    "## 17. Saving Generated Valid SMILES to a CSV File\n",
    "\n",
    "*(Corresponds to the cell saving generated SMILES to `molecules_generated.csv`)*\n",
    "\n",
    "The unique and valid generated SMILES strings are saved into a CSV file for later use, analysis, or evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
